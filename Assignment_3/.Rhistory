return(summation_term/(2*m))
}
cost = cost_function(x,y,no_of_rows,thetha)
#Cost function ends*****
#Checking cost_function correctness begins*****
#This was used to check when all thethas were zero.
sum =0
for(i in 1:length(y)){
sum = sum +(y[i])^2
}
print(sum/(2*length(y)))
#Checking cost_function correctness ends*****
#Gradient Descent begins*****
for(iter in 1:iterations){
derivative1 = 0
for(i in 1:no_of_rows){
derivative1 = derivative1 + ((x[i:i,] %*% thetha) - y[i])*x[i:i,1:1]
}
derivative1 = derivative1/no_of_rows
temp1 = thetha[1] - (alpha * derivative1)
derivative2 = 0
for(i in 1:no_of_rows){
derivative2 = derivative2 + ((x[i:i,] %*% thetha) - y[i])*x[i:i,2:2]
}
derivative2 = derivative2/no_of_rows
temp2 = thetha[2] - (alpha * derivative2)
thetha[1] = temp1
thetha[2] = temp2
cost = cost_function(x,y,no_of_rows,thetha)
contour_x = c(contour_x,thetha[1])
contour_y = c(contour_y,thetha[2])
contour_z = c(contour_z,cost)
}
#Gradient Descent ends*****
#1.2 begins******
plot(x = x[,2], y = y,xlab = "Horsepower", ylab = "mpg")
abline(coef = thetha)
#1.2 ends******
thetha
data_horsepower2 = data["horsepower"]
data_horsepower2[393:393,1:1] = c(220)
data_horsepower2 = scale(data_horsepower2)
data_horsepower2 = as.matrix(data_horsepower2)
x_0_2 = matrix(data=1,nrow = no_of_rows+1, ncol = 1)
x2   = cbind(x_0_2,data_horsepower2)
colnames(x2) = NULL
x2[393,] %*% thetha
coefficients = solve(t(x) %*% x) %*% t(x) %*% y
coefficients[1]
coefficients[2]
plot_ly(x = contour_x, y= contour_y, z= contour_z, type = "contour")
set.seed(123)
#a
x=rnorm(100)
x
#b
eps=rnorm(100,0,0.5)
#c
y=-1+0.5*x+eps
length(y)
length(y2)
#theta0= -1 and theta1=0.5
#d
plot(x,y)
#e
as.matrix(x)
as.matrix(y)
lm.fit=lm(y~x)
lm.fit
fit1=lm.fit$coefficients
#f
abline(lm.fit, col="red")
abline(-1,0.5,col="blue")
legend("topleft",c( "least square","regression"),col=c("red","blue"),lty = c(1,1))
#g
plot(x,y)
non_linear_fit=lm(y~x+I(x^2))
lines(sort(x), fitted(non_linear_fit)[order(x)], col='green')
#h
eps2=rnorm(100,0,0.3)
y2=-1+0.5*x+eps2
plot(x,y2)
as.matrix(y2)
as.matrix(x)
lm.fit2=lm(y2~x)
lm.fit2
fit2=lm.fit2$coefficients
abline(lm.fit2, col="red")
abline(-1,0.5,col="blue")
legend("topleft",c( "least square","regression"),col=c("red","blue"),lty = c(1,1))
#i
eps3=rnorm(100,0,0.8)
y3=-1+0.5*x+eps3
plot(x,y3)
as.matrix(y3)
as.matrix(x)
lm.fit3=lm(y3~x)
lm.fit3
fit3=lm.fit3$coefficients
abline(lm.fit3, col="red")
abline(-1,0.5,col="blue")
legend("topleft",c( "least square","regression"),col=c("red","blue"),lty = c(1,1))
#j
confint(lm.fit)
confint(lm.fit2)
confint(lm.fit3)
#14.a begins*****
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
#14.a ends*****
#14.b begins*****
cor(x1, x2)
plot(x1,x2)
#14.b ends*****
#14.c begins*****
model = lm(y~x1+x2)
model
summary(model)
#14.d begins*****
model= lm(y ~ x1)
model
summary(model)
#14.d ends*****
#14.e begins*****
model= lm(y ~ x2)
model
summary(model)
library(MASS)
attach(Boston)
fit_chas = lm(crim ~ chas)
summary(fit_chas)
#15.a ends*****
names(Boston)
predictors = names(Boston)
predictors = predictors[-1]
predictors
data = Boston
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
plot(data[predictors[i]],crim)
}
data
View(data)
predictors[i]
plot(x = zn,y=crim)
zn
crim
plot(x = data[zn],y=data[crim])
plot(x = data["zn"],y=data["crim"])
data["zn"]
data["crim"]
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
plot(data[predictors[i]],data["crim"])
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
plot(data[predictors[i]],data["crim"])
}
class(data[predictors[i]])
class(data["crim"])
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
lm(crim ~ data[predictors[i]], data = Boston)
#plot(data[predictors[i]],data["crim"])
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
lm(crim ~ predictors[i], data = Boston)
#plot(data[predictors[i]],data["crim"])
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
y = data["crim"]
lm(y ~ x)
#plot(data[predictors[i]],data["crim"])
}
class(x)
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
lm(y ~ x)
#plot(data[predictors[i]],data["crim"])
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
#lm(y ~ x)
plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
#lm(y ~ x)
plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
summary(model)
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
summary(model)
#plot(x,y)
}
summary(model)
install.packages("alr4")
library(alr4)
scatterplotMatrix(~data["zn"])
scatterplotMatrix(~crim+zn,data=Boston,smooth=FALSE)
scatterplot(data)
pairs(~mpg+disp+drat+wt,data=mtcars,
main="Simple Scatterplot Matrix")
View(data)
pairs(~crim+zn+indus+chas,data=Boston,
main="Simple Scatterplot Matrix")
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
summary(model)
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ data[predictors[i]])
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ as.matrix(data[predictors[i]])
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ as.matrix(data[predictors[i]]))
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:1){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:length(predictors)){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
print(summary(model))
#plot(x,y)
}
data = Boston
predictors = names(Boston)
predictors = predictors[-1]
for(i in 1:length(predictors)){
x = data[predictors[i]]
x = as.matrix(x)
y = data["crim"]
y = as.matrix(y)
model = lm(y ~ x)
print(summary(model))
#plot(x,y)
}
model = lm(crim~chas,data=Boston)
summary(model)
getwd()
#Read data section begins*****
require("ISLR")
data = Auto
y = data["mpg"]
y = as.matrix(y)
#Find the median
median_y = median(y)
for(i in 1:length(y)){
if(y[i]>median_y){
y[i] = 1
}
else{
y[i] = 0
}
}
library("ISLR")
install.packages("ISLR")
#Read data section begins*****
require("ISLR")
data = Auto
y = data["mpg"]
y = as.matrix(y)
#Find the median
median_y = median(y)
for(i in 1:length(y)){
if(y[i]>median_y){
y[i] = 1
}
else{
y[i] = 0
}
}
data$mpg = NULL
data$name= NULL
data$acceleration = NULL
data$year = NULL
data$origin = NULL
data$name = NULL
data     = scale(data)
no_of_rows = dim(data)[1]
x_0 = matrix(data=1,nrow = no_of_rows, ncol = 1)
x   = cbind(x_0,as.matrix(data))
no_of_cols = dim(x)[2]
colnames(x) = NULL
#Read data section ends*****
#Paramters section begins*****
#alpha is the learning rate
alpha = 3
iterations = 800
#Number of features considered will be the number of rows
#of thetha and each is initialized to zero. dim(thetha)= 2*1
thetha = matrix(data=0,nrow =dim(x)[2] ,ncol = 1)
#Paramters section ends*****
#Cost function begins*****
cost_function = function(x,y,m,thetha){
summation_term = 0
for(i in 1:m){
h_thetha_x = exp(x[i:i,]%*% thetha)/(1 + exp(x[i:i,]%*% thetha) )
summation_term = summation_term + (-y[i]*log(h_thetha_x) - (1-y[i])*log(1-h_thetha_x))
}
print(summation_term/(2*m))
}
cost_function(x,y,no_of_rows,thetha)
#Cost function ends*****
#Gradient Descent begins*****
for(iter in 1:iterations){
derivative = c()
for(all_features in 1:no_of_cols){
derivative_temp = 0
for(i in 1:no_of_rows){
h_thetha_x = exp(x[i:i,]%*% thetha)/(1 + exp(x[i:i,]%*% thetha) )
derivative_temp = derivative_temp + (h_thetha_x - y[i])*x[i:i,all_features:all_features]
}#for(i) ends here
derivative_temp = derivative_temp /no_of_rows
thetha[all_features] = thetha[all_features] - (alpha * derivative_temp)
}#for(all_features) ends here
cost_function(x,y,no_of_rows,thetha)
}
#Gradient Descent ends*****
#Error calculation begins*****
mis_classification_count = 0
for(i in length(y)){
h_thetha_x = exp(x[i:i,]%*% thetha)/(1 + exp(x[i:i,]%*% thetha) )
if(h_thetha_x >=0.5){
h_thetha_x = 1
if(h_thetha_x !=y[i]){
mis_classification_count = mis_classification_count + 1
}
}
else{
h_thetha_x = 0
if(h_thetha_x !=y[i]){
mis_classification_count = mis_classification_count + 1
}
}
}
#Error calculation ends*****
#For 3.4
data = Auto
data$mpg = NULL
data$name= NULL
data$acceleration = NULL
data$year = NULL
data$origin = NULL
data$name = NULL
newRow = data.frame(cylinders=8, displacement=340, horsepower=200,weight=3500)
data = rbind(data,newRow)
data     = scale(data)
no_of_rows = dim(data)[1]
x_0 = matrix(data=1,nrow = no_of_rows, ncol = 1)
x_1   = cbind(x_0,as.matrix(data))
no_of_cols = dim(x_1)[2]
colnames(x_1) = NULL
ans = exp(x_1[393:393,]%*% thetha)/(1 + exp(x_1[393:393,]%*% thetha))
if(ans<0.5){
print("0")
}
if(ans>=0.5){
print("1")
}
#For 3.4 ends
mis_classification_count
median((y))
View(x)
set.seed(1)
#a
x=rnorm(100)
x
#b
eps=rnorm(100,0,0.5)
#c
y = -1 + 0.5 * x + eps
length(y)
#theta0= -1 and theta1=0.5
#d
plot(x,y)
#e
as.matrix(x)
as.matrix(y)
model =lm(y~x)
parameters=model$coefficients
#f
abline(model, col="red")
abline(-1,0.5,col="black")
legend("topleft",c( "least square","regression"),col=c("red","black"),lty = c(1,1))
#g
plot(x,y)
model=lm(y~x+I(x^2))
summary(model)
abline(model)
abline(model)
getwd()
